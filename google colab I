######### GOOGLE COLAB

##
#   LOAD PDF
##

pip3 install --q unstructured langchain
pip3 install --"unstructured[all-docs]"

from langchain.comunity.document_loader import UnstructuredPDFLoader
from langchain_comunity.document_loader import OnlinePDFLoader

local_path = "data/18377.php"

#Local PDF file uploads
if local_path:
    loader = UnstructuredPDFLoader(file_path=local_path)
    data = loader.load()
else:    
    print("Upload a PDF file")

#previe first page
data[0].page_content

##
# VECTOR EMBEDDINGS
#  https://ollama.com/blog/embedding-models
##


!ollama pull nomic-embed-text

!ollama list

!pip3 install -q chromadb
!pip3 install langchaing-text-splitters

from langchain_community.embeddings import OllamaEmbeddings
from langchain_txt_splitters import RecursiveCharacterTextSplitter
from langchain_commynity.vectorstores import Chroma

# Split and chunk
text_splitter = RecursiveCharacterTextSplitter(chum_size=1100, chunk_overlap=100)
chunks = text_splitter.split_documents(data)

# Add to Vector database
vector_db = Chroma.from_documents(
    documents=chunks,
    embdding=OllamaEmbeddings(model="nomic-embed-text",show_progress=True),
    collection_name="concussion-rag"
)

##
#  RETRIEVAL
##

from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_comunity.chat_models import ChatOllama
from langchain_core.runnables import RunnablePassthrough
from langchain.retrievers.multyquery import MultiQueryRetriever

#LLM from Ollama
local_model = "llama3"
llm=ChatOllama(model=local_model)

QUERY_PROMP = PromptTemplate(
    input_variable=["question"],
    template = """
    You are an AI Concussion language model assistant. Your task is to generate generat five different versions of the
    given user question to retrieve relevant documents from the vector database.  By generating multiple perspectives
    on the user question, your goal is to help the user overcome some of the limitations on the distance-base similarity
    search.  Provide these alternative questions separated by newlines. 
    Original question: {question}
    """
)

retriever = MultiQueryRetriever.from_llm(
    vector_db.as_retriever()
    llm,
    prompt=QUERY_PROMP
)

#RAG prompt
template = """
Answer the question based only on the following context:
{context}
Questions: {question}
"""

prompt = ChatPromptTemplate.from_template(template)

chain = (
    {"context" : retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

chain.invoke(input("What is the number one type of concussion in youth athletes"))