{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb736df-79ec-469a-822a-f5bbb1c78c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Data directory: .DS_Store\n",
      "--> Data subdirectory: data/.DS_Store\n",
      "-> Data directory: pdf\n",
      "--> Data subdirectory: data/pdf\n",
      "---> Found PDF file: data/pdf/Canadian-Guideline-on-Concussion-in-Sport-2nd-edition-2024.pdf\n",
      "PDF loaded successfully: data/pdf/Canadian-Guideline-on-Concussion-in-Sport-2nd-edition-2024.pdf\n",
      "---> Found PDF file: data/pdf/Policy-on-Management-of-Sports-Related-Concussion-2024-2025.pdf\n",
      "PDF loaded successfully: data/pdf/Policy-on-Management-of-Sports-Related-Concussion-2024-2025.pdf\n",
      "---> Found PDF file: data/pdf/18377.pdf\n",
      "PDF loaded successfully: data/pdf/18377.pdf\n",
      "-> Data directory: json\n",
      "--> Data subdirectory: data/json\n",
      "Skipping non-PDF file: book.json\n",
      "-> Data directory: csv\n",
      "--> Data subdirectory: data/csv\n",
      "Skipping non-PDF file: NFL Head Injured Players.csv\n",
      "-> Data directory: text\n",
      "--> Data subdirectory: data/text\n",
      "Total documents loaded: 1118140\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 70\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal documents loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data_array)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Create a Document object from the text content in 'data'\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# If 'data' is a string, wrap it in a Document object\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m documents \u001b[38;5;241m=\u001b[39m [\u001b[43mDocument\u001b[49m(page_content\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data_array]  \u001b[38;5;66;03m# 'data' here should be a string or a list of strings\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Split text into chunks\u001b[39;00m\n\u001b[1;32m     73\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Document' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "import os\n",
    "import pdfplumber\n",
    "\n",
    "# Initialize the data array\n",
    "data_array = []\n",
    "\n",
    "# Set the protocol buffers environment variable (if needed)\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "# Directory containing the PDF files\n",
    "directory_path = \"data/\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_path):\n",
    "    print(f\"Directory {directory_path} does not exist.\")\n",
    "else:\n",
    "    # Loop through all directories in the 'data' directory\n",
    "    for dirname in os.listdir(directory_path):\n",
    "        print(f\"-> Data directory: {dirname}\")\n",
    "\n",
    "        # Full path for the subdirectory\n",
    "        full_path = os.path.join(directory_path, dirname)\n",
    "        print(f\"--> Data subdirectory: {full_path}\")\n",
    "\n",
    "        # Check if it's a directory\n",
    "        if not os.path.isdir(full_path):\n",
    "            continue\n",
    "\n",
    "        # Loop through all files in the subdirectory\n",
    "        for filename in os.listdir(full_path):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                # Construct the full file path for the PDF\n",
    "                local_path = os.path.join(full_path, filename)\n",
    "                print(f\"---> Found PDF file: {local_path}\")\n",
    "\n",
    "                try:\n",
    "  \n",
    "\n",
    "                    with pdfplumber.open(local_path) as pdf:\n",
    "                        pdf_data = \"\"\n",
    "                        for page in pdf.pages:\n",
    "                            pdf_data += page.extract_text()\n",
    "\n",
    "                    # Check if pdf_data is empty or None\n",
    "                    if pdf_data:\n",
    "                        # Append the individual documents from the pdf_data\n",
    "                        data_array.extend(pdf_data)\n",
    "                        print(f\"PDF loaded successfully: {local_path}\")\n",
    "                    else:\n",
    "                        print(f\"Warning: No data extracted from PDF {local_path}\")\n",
    "                except Exception as e:\n",
    "                    # Print any errors that occur during PDF loading\n",
    "                    print(f\"Error loading PDF {local_path}: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"Skipping non-PDF file: {filename}\")\n",
    "\n",
    "    # Print the total number of documents loaded (not just PDFs)\n",
    "    print(f\"Total documents loaded: {len(data_array)}\")\n",
    "\n",
    "# Create a Document object from the text content in 'data'\n",
    "# If 'data' is a string, wrap it in a Document object\n",
    "documents = [Document(page_content=d) for d in data_array]  # 'data' here should be a string or a list of strings\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Text split into {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eafda9d-3d93-4a6d-98cf-d6aaedf8dd55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
